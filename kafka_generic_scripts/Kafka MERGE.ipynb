{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d84b227f-f446-4139-b734-14653c183284",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cc1e6b3-8fee-43fa-bbae-b21355ad883a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "TABLE_NAME = dbutils.widgets.get(\"TABLE_NAME\")\n",
    "string_schema = dbutils.widgets.get(\"QUERY\")\n",
    "ENABLED = dbutils.widgets.get(\"ENABLED\")\n",
    "SCHEMA = dbutils.widgets.get(\"SCHEMA\")\n",
    "primary_key = dbutils.widgets.get(\"PK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dd1606c-4113-46c1-bcc1-d1a035c564e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #variable initialization Same principle with Azure DLS2 but with no need for raw zone and other paths, no clean up requirements as well \n",
    "# see create notes, only difference here is we dont read the 'r' data from debezium\n",
    "\n",
    "\n",
    "# TABLE_NAME= \"TABLE1_\"\n",
    "# string_schema = \"\"\"\n",
    "# SELECT \n",
    "#         after.SomeID AS SomeID\n",
    "#        ,after.Comments         AS Comments\n",
    "#        ,after.LoginID          AS LoginID\n",
    "#        ,after.OpenedDT         AS OpenedDT\n",
    "#        ,after.SavedDT          AS SavedDT\n",
    "#        ,after.OriginalLoginID  AS OriginalLoginID\n",
    "#        ,after.EditedMSID       AS EditedMSID\n",
    "#        ,after.ApprovalLoginID  AS ApprovalLoginID\n",
    "#        ,ts_ms as kafka_ts\n",
    "#        ,op as op\n",
    "#         ,row_number() over (partition by after.SomeID order by ts_ms desc) rn\n",
    "#     FROM tmp_{}\n",
    "#     where op != 'r'\n",
    "#     order by ts_ms asc\n",
    "# \"\"\"\n",
    "# ENABLED = 'TRUE'\n",
    "# SCHEMA = 'some_schema'\n",
    "\n",
    "# primary_key = 'SomeID'\n",
    "\n",
    "timestamp_key = 'kafka_ts'\n",
    "delta_table_path = f'/mnt/databricks/DELTA/{SCHEMA}/{TABLE_NAME}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3960a532-0474-4dd5-a391-fd22d51e7836",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# reads the checkpoint table \n",
    "\n",
    "latest_ts = spark.sql(f\"\"\"select last_sync_ts from default.control_table \n",
    "                         where table_name=='{TABLE_NAME}' and schema_name == '{SCHEMA}'\"\"\").collect()[0]['last_sync_ts']\n",
    "\n",
    "# read the kafka stream base on the latest timestamp from the checkpoint table, this way, only the needed topics or latest rows are requested \n",
    "\n",
    "raw = (spark.read\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafkabrokeraddress:9092\")\n",
    "  .option(\"subscribe\", \"topic\")\n",
    "  .option(\"startingTimestamp\", int(latest_ts)+1)\n",
    "  .option(\"endingTimestamp\", \"9000000000000\")\n",
    "  .option(\"kafka.group.id\", \"dbs_consumer1\")\n",
    "  .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a04446ed-e9c9-424c-a1ef-f9c2eb47ea63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# this line of code reads the debezium payload from kafka\n",
    "\n",
    "df = raw.selectExpr(\"\"\"\n",
    "                   from_json(cast(value as string), \"STRUCT<payload: STRUCT<\n",
    "                      after: \n",
    "                          MAP<STRING, STRING>,\n",
    "                      op: STRING,\n",
    "                      ts_ms: DOUBLE\n",
    "                      >\n",
    "                      >\").payload.after as after\"\"\", \n",
    "                      \"\"\"from_json(cast(value as string), \"STRUCT<payload: STRUCT<\n",
    "                      after: \n",
    "                          MAP<STRING, STRING>,\n",
    "                      op: STRING,\n",
    "                      ts_ms: DOUBLE\n",
    "                      >\n",
    "                      >\").payload.ts_ms as ts_ms\"\"\",\n",
    "                      \"\"\"from_json(cast(value as string), \"STRUCT<payload: STRUCT<\n",
    "                      after: \n",
    "                          MAP<STRING, STRING>,\n",
    "                      op: STRING,\n",
    "                      ts_ms: DOUBLE\n",
    "                      >\n",
    "                      >\").payload.op as op\"\"\",\n",
    "                      'cast(timestamp as double) * 1000 as sync_ts' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efb1c16f-f08e-462a-a3c1-311c336a4efe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "max_value = df.selectExpr(\"max(sync_ts) as max_value\").collect()[0]['max_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2accebf-f0ff-46f9-8889-7d73d265f3ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MeasurementSetID', 'Comments', 'LoginID', 'OpenedDT', 'SavedDT', 'OriginalLoginID', 'EditedMSID', 'ApprovalLoginID', 'kafka_ts']\n"
     ]
    }
   ],
   "source": [
    "# do the actual merge \n",
    "\n",
    "import datetime\n",
    "#capture the current timestamp - store it on a variable called run_timestamp\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import *\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
    "\n",
    "\n",
    "df_changes = df\n",
    "\n",
    "df_changes.createOrReplaceTempView('tmp_' + TABLE_NAME)\n",
    "\n",
    "extracted_df = spark.sql(string_schema.format(TABLE_NAME))\n",
    "\n",
    "cleaned_df = extracted_df.filter(col(\"rn\") == 1)\n",
    "\n",
    "\n",
    "col_list = delta_table.toDF().columns\n",
    "print(col_list)\n",
    "merge_action = {f\"target.{colName}\": f\"source.{colName}\" for colName in col_list}\n",
    "\n",
    "\n",
    "merge_condition = (col(\"target.{}\".format(primary_key)) == col(\"source.{}\".format(primary_key)))\n",
    "insert_condition = (col(\"source.op\") == \"c\")\n",
    "update_condition = (col(\"source.op\") == \"u\")\n",
    "delete_condition = (col(\"source.op\") == \"d\")\n",
    "\n",
    "# Merge the extracted_df into delta_table\n",
    "delta_table.alias(\"target\").merge(\n",
    "    cleaned_df.alias(\"source\"),\n",
    "    merge_condition\n",
    ").whenMatchedUpdate(\n",
    "    condition=update_condition,\n",
    "    set=merge_action\n",
    ").whenMatchedDelete(\n",
    "    condition=delete_condition\n",
    ").whenNotMatchedInsert(\n",
    "    values=merge_action\n",
    ").execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94675e2b-46e7-4630-bfbc-5321672eed03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[7]: DataFrame[num_affected_rows: bigint]"
     ]
    }
   ],
   "source": [
    "# update the control table , the last_sync_ts is the timestamp from the kafka offset ts , so that on the next run, only data from that ts in ms is captured\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "          update default.control_table set last_sync_ts = {max_value}\n",
    "          where schema_name == '{SCHEMA}' and table_name == '{TABLE_NAME}'\n",
    "          \"\"\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3767748449357015,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Kafka MERGE",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
