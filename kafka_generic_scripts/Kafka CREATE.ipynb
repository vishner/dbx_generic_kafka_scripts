{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d84b227f-f446-4139-b734-14653c183284",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96dcd1dc-3026-4a2e-802c-6d875e737e62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "TABLE_NAME = dbutils.widgets.get(\"TABLE_NAME\")\n",
    "string_schema = dbutils.widgets.get(\"QUERY\")\n",
    "ENABLED = dbutils.widgets.get(\"ENABLED\")\n",
    "SCHEMA = dbutils.widgets.get(\"SCHEMA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dd1606c-4113-46c1-bcc1-d1a035c564e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\vishn\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/vishn/AppData/Local/Programs/Python/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# this is a sample parameter base on the table name TABLE1_\n",
    "# string_schema is how you will query the json object formated from a dataframe, see step by step movement of dataframe for more information\n",
    "# query need to be partitioned by a primary key so only latest data are inserted\n",
    "# ENABLED is a job flag for databricks\n",
    "# Schema is where the table will be created, by default the table is created on a mounted folder under mnt/databricks/{schema}/{tablename}\n",
    "# make sure a control table exists for tracking last processed timestamp\n",
    "\n",
    "\n",
    "# TABLE_NAME= \"TABLE1_\"\n",
    "# string_schema = \"\"\"\n",
    "# SELECT \n",
    "#         after.SomeID AS SomeID\n",
    "#        ,after.Comments         AS Comments\n",
    "#        ,after.LoginID          AS LoginID\n",
    "#        ,after.OpenedDT         AS OpenedDT\n",
    "#        ,after.SavedDT          AS SavedDT\n",
    "#        ,after.OriginalLoginID  AS OriginalLoginID\n",
    "#        ,after.EditedMSID       AS EditedMSID\n",
    "#        ,after.ApprovalLoginID  AS ApprovalLoginID\n",
    "#        ,ts_ms as kafka_ts,\n",
    "#         row_number() over (partition by after.SomeID order by ts_ms desc) rn\n",
    "#     FROM tmp_{}\n",
    "#     where op != 'd'\n",
    "#     order by ts_ms asc\n",
    "# \"\"\"\n",
    "# ENABLED = 'TRUE'\n",
    "# SCHEMA = 'some_schema'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3960a532-0474-4dd5-a391-fd22d51e7836",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw = (spark.read\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafkabrokeraddress:9092\")\n",
    "  .option(\"subscribe\", \"topic\")\n",
    "  .option(\"startingOffset\", \"earliest\")\n",
    "  .option(\"endingOffset\", \"latest\")\n",
    "  .option(\"kafka.group.id\", \"dbs_consumer1\")\n",
    "  .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a04446ed-e9c9-424c-a1ef-f9c2eb47ea63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max as sql_max\n",
    "df = raw.selectExpr(\"\"\"\n",
    "                   from_json(cast(value as string), \"STRUCT<payload: STRUCT<\n",
    "                      after: \n",
    "                          MAP<STRING, STRING>,\n",
    "                      op: STRING,\n",
    "                      ts_ms: DOUBLE\n",
    "                      >\n",
    "                      >\").payload.after as after\"\"\", \n",
    "                      \"\"\"from_json(cast(value as string), \"STRUCT<payload: STRUCT<\n",
    "                      after: \n",
    "                          MAP<STRING, STRING>,\n",
    "                      op: STRING,\n",
    "                      ts_ms: DOUBLE\n",
    "                      >\n",
    "                      >\").payload.ts_ms as ts_ms\"\"\",\n",
    "                      \"\"\"from_json(cast(value as string), \"STRUCT<payload: STRUCT<\n",
    "                      after: \n",
    "                          MAP<STRING, STRING>,\n",
    "                      op: STRING,\n",
    "                      ts_ms: DOUBLE\n",
    "                      >\n",
    "                      >\").payload.op as op\"\"\",\n",
    "                      'cast(timestamp as double) * 1000 as sync_ts' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "859e249b-bbf6-4160-86ca-5e3381eb1a2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1698243742948.0\n"
     ]
    }
   ],
   "source": [
    "max_value = df.selectExpr(\"max(sync_ts) as max_value\").collect()[0]['max_value']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2accebf-f0ff-46f9-8889-7d73d265f3ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# CLEAN DELTA LOCATION FIRST\n",
    "if(ENABLED == 'TRUE'):\n",
    "\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder.appName(\"Create Delta Table\").getOrCreate()\n",
    "\n",
    "    df.createOrReplaceTempView('tmp_' + TABLE_NAME)\n",
    "\n",
    "    extracted_df = spark.sql(string_schema.format(TABLE_NAME))\n",
    "    extracted_df = extracted_df.filter(col(\"rn\")== 1)\n",
    "    extracted_df = extracted_df.drop(\"rn\")\n",
    "    display(extracted_df)\n",
    "    extracted_df.write.format(\"delta\").mode(\"overwrite\").save(f'/mnt/databricks/DELTA/{SCHEMA}/{TABLE_NAME}')\n",
    "\n",
    "\n",
    "    final_table_name = f\"{SCHEMA}.{TABLE_NAME}\"\n",
    "\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "        DROP TABLE IF EXISTS {final_table_name}\n",
    "    \"\"\")\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE SCHEMA IF NOT EXISTS {SCHEMA}\n",
    "    \"\"\")\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {final_table_name}\n",
    "        USING DELTA\n",
    "        LOCATION '/mnt/databricks/DELTA/{SCHEMA}/{TABLE_NAME}'\n",
    "    \"\"\")\n",
    "\n",
    "    # if CDC == \"TRUE\":\n",
    "    #     spark.sql(f\"\"\"\n",
    "    #         ALTER TABLE {final_table_name} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "    #     \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4950555-4444-43dc-b7c5-418447f9c3e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "          delete from default.control_table \n",
    "          where schema_name = '{SCHEMA}' and table_name = '{TABLE_NAME}'\n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94675e2b-46e7-4630-bfbc-5321672eed03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[29]: DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "          insert into  default.control_table values ('{SCHEMA}', '{TABLE_NAME}', {max_value})\n",
    "          \"\"\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Kafka CREATE",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
